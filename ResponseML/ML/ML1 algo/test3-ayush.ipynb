{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import the necessary libraries and getting machine ready\n",
    "\n",
    "1) Pandas- open source data analysis library for providing easy-to-use data structures and data analysis tools\n",
    "\n",
    "2) Numpy - general-purpose array-processing package. It provides a high-performance multidimensional array object, and tools for working with these arrays. It is the fundamental package for scientific computing with Python.\n",
    "\n",
    "3) BeautifulSoup- Python library for pulling data out of HTML and XML files.\n",
    "\n",
    "4) unicodedata- This module provides access to the Unicode Character Database (UCD) which defines character properties for all Unicode characters. \n",
    "\n",
    "5) contractions- Fixes contractions such as `you're` to you `are`\n",
    "\n",
    "6) re- This module provides regular expression matching operations similar to those found in Perl.\n",
    "\n",
    "7) nltk- NLTK stands for Natural Language Toolkit. This toolkit is one of the most powerful NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages \n",
    "\n",
    "8) RegexpTokenizer- splits a string into substrings using a regular expression.\n",
    "\n",
    "9) WordNetLemmatizer- Lemmatize using WordNet's built-in morphy function.Returns the input word unchanged if it cannot be found in WordNet.\n",
    "\n",
    "10) CountVectorizer- Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "11) TfidfVectorizer- Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "12) sklearn.model_selection- Split arrays or matrices into random train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Analyze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file into pandas\n",
    "df=pd.read_excel(\"dataset2_11k.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-  Pandemonium In Aba As Woman Delivers Baby W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#OTRAMETLIFE ' I SWEAR TO GOD I DIDNT EVEN RE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Lord Forgive Me For Body Bagging @MeekMi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no pharrell  only YOU can prevent forest fire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-- small bag from the bottom the wounded hero ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  result\n",
       "0  -  Pandemonium In Aba As Woman Delivers Baby W...       0\n",
       "1   #OTRAMETLIFE ' I SWEAR TO GOD I DIDNT EVEN RE...       0\n",
       "2   Dear Lord Forgive Me For Body Bagging @MeekMi...       0\n",
       "3   no pharrell  only YOU can prevent forest fire...       0\n",
       "4  -- small bag from the bottom the wounded hero ...       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>Zouma has just absolutely flattened that guy ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10739</th>\n",
       "      <td>Zouma! Runaway train. Absolutely flattened the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10740</th>\n",
       "      <td>ו_? New Ladies Shoulder Tote #Handbag Faux Lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10741</th>\n",
       "      <td>ו₪} New Ladies Shoulder Tote #Handbag Faux Lea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10742</th>\n",
       "      <td>וָMGN-AFRICAו¨ pin:263789F4 וָ Correction: Ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  result\n",
       "10738    Zouma has just absolutely flattened that guy ??       0\n",
       "10739  Zouma! Runaway train. Absolutely flattened the...       0\n",
       "10740  ו_? New Ladies Shoulder Tote #Handbag Faux Lea...       0\n",
       "10741  ו₪} New Ladies Shoulder Tote #Handbag Faux Lea...       0\n",
       "10742  וָMGN-AFRICAו¨ pin:263789F4 וָ Correction: Ten...       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result\n",
       "0    8230\n",
       "1    2513\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "df.result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10743.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.233920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.423341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             result\n",
       "count  10743.000000\n",
       "mean       0.233920\n",
       "std        0.423341\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate summary statistics, excluding Nan values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10743 entries, 0 to 10742\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   10743 non-null  object\n",
      " 1   result  10743 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 168.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# process summary of a dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     0\n",
       "result    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Preprocessing of data\n",
    "To preprocess your text simply means to bring your text into a form that is predictable and analyzable for your task. \n",
    "\n",
    "NLP is short for Natural Language Processing.\n",
    "\n",
    "Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.\n",
    "\n",
    "The workflow that will be followed is:\n",
    "\n",
    "#### 1) Change to lower case:\n",
    "Lowercasing is one of the effective forms. Variation in input capitalization can give us different output. For example: Canada and canada is the same but the neural network might not be able to make that out.\n",
    "\n",
    "#### 2) Remove HTML tags: \n",
    "Since we have got data through web scraping, the text might contain a lot of noise. HTML tags do not add much value towards understanding and analyzing text so we will remove the HTML tags.\n",
    "\n",
    "#### 3) Remove accented characters:\n",
    "Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example — converting é to e.\n",
    "\n",
    "#### 4) Remove URL:\n",
    "The url links are of n use and will not help so we will remove it and replace with \"\". Basically just wiping it.\n",
    "\n",
    "#### 5) Expanding contractions:\n",
    "Contractions are shortened version of words or syllables. In case of English contractions, they are often created by removing one of the vowels from the word. Examples would be, do not to don’t and I would to I’d. Converting each contraction to its expanded, original form helps with text standardization.\n",
    "\n",
    "#### 6) Removing special characters (punctuations, hashtags and @):\n",
    "Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them.\n",
    "\n",
    "#### 7) Tokenization:\n",
    "This breaks up the strings into a list of words or pieces based on a specified pattern using Regular Expressions aka RegEx. The pattern I chose to use this time (r'\\w') also removes punctuation and is a better option for this data in particular. \n",
    "\n",
    "#### 8) Remove stop words:\n",
    "We imported a list of the most frequently used words from the NL Toolkit with from nltk.corpus import stopwords. There are 179 English words, including ‘i’, ‘me’, ‘my’, ‘myself’, ‘we’, ‘you’, ‘he’, ‘his’, for example. We usually want to remove these because they have low predictive power. \n",
    "\n",
    "#### 9) Lemmatization or stemming\n",
    "Both tools shorten words back to their root form. Stemming is a little more aggressive. It cuts off prefixes and/or endings of words based on common ones. It can sometimes be helpful, but not always because often times the new word is so much a root that it loses its actual meaning. Lemmatizing, on the other hand, maps common words into one base. Unlike stemming though, it always still returns a proper word that can be found in the dictionary. \n",
    "we will compare to see which one works better. For stemming we will use snowball stemmer as it is proven to give better than than porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-  pandemonium in aba as woman delivers baby w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#otrametlife ' i swear to god i didnt even re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear lord forgive me for body bagging @meekmi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no pharrell  only you can prevent forest fire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-- small bag from the bottom the wounded hero ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  result\n",
       "0  -  pandemonium in aba as woman delivers baby w...       0\n",
       "1   #otrametlife ' i swear to god i didnt even re...       0\n",
       "2   dear lord forgive me for body bagging @meekmi...       0\n",
       "3   no pharrell  only you can prevent forest fire...       0\n",
       "4  -- small bag from the bottom the wounded hero ...       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercasing\n",
    "df['title']=df['title'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     html_free=soup.get_text()\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m html_free\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]=\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      4\u001b[39m     html_free=soup.get_text()\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m html_free\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]=df[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mremove_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mremove_html\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_html\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     soup=\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlxml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     html_free=soup.get_text()\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m html_free\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\bs4\\__init__.py:250\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    248\u001b[39m     builder_class = builder_registry.lookup(*features)\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    251\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    252\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features))\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Remove HTML tags\n",
    "def remove_html(text):\n",
    "    soup=BeautifulSoup(text,'lxml')\n",
    "    html_free=soup.get_text()\n",
    "    return html_free\n",
    "\n",
    "df['title']=df['title'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing accented characters\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "df['title']=df['title'].apply(lambda x: remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "df['title']=df['title'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "#df.iloc[9977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding contractions   \n",
    "df['title']=df['title'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "# The above statement tokenizes the sentence in such a \n",
    "# way that if word is \"I've\" the token formed is [I have] after \n",
    "# expanding. So we use the below command to contract the tokens\n",
    "# back to sentence\n",
    "df['title']=[' '.join(map(str,l)) for l in df['title']]\n",
    "#df.iloc[9977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the hashtags and @ retaining the meaningful words\n",
    "def hashtag(x):\n",
    "    text=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "    return text\n",
    "\n",
    "df['title']=df['title'].apply(lambda x: hashtag(x))\n",
    "#df.iloc[9975]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pandemonium, in, aba, as, woman, delivers, ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[otrametlife, i, swear, to, god, i, did, not, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dear, lord, forgive, me, for, body, bagging, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[no, pharrell, only, you, can, prevent, forest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[small, bag, from, the, bottom, the, wounded, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  result\n",
       "0  [pandemonium, in, aba, as, woman, delivers, ba...       0\n",
       "1  [otrametlife, i, swear, to, god, i, did, not, ...       0\n",
       "2  [dear, lord, forgive, me, for, body, bagging, ...       0\n",
       "3  [no, pharrell, only, you, can, prevent, forest...       0\n",
       "4  [small, bag, from, the, bottom, the, wounded, ...       0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer= RegexpTokenizer(r'\\w+')\n",
    "df['title']=df['title'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayush/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayush/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Remove stop words\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_stopwords\u001b[39m(text):\n\u001b[32m      5\u001b[39m     words=[w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\corpus\\util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayush/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayush\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words=[w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "df['title']=df['title'].apply(lambda x: remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    lem_text=[lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "df['title']=df['title'].apply(lambda x: word_lemmatizer(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "def word_stemmer(text):\n",
    "    stem_text=\" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text\n",
    "\n",
    "df['title']=df['title'].apply(lambda x: word_stemmer(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Splitting of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances to learn from\n",
    "X=df.title \n",
    "# target/responses the model is trying to learn to predict\n",
    "y=df.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 instances\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 target/responses\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Splitting the dataset into train and test\n",
    "In machine learning we usually split our data into two subsets: training data and testing data (and sometimes to three: train, validate and test), and fit our model on the train data, in order to make predictions on the test data.\n",
    "\n",
    "The SciKit library provides a tool, called the Model Selection library. There’s a class in the library which is, aptly, named ‘train_test_split.’ Using this we can easily split the dataset into the training and the testing datasets in various proportions.\n",
    "\n",
    "#### sklearn.model_selection.train_test_split(*arrays, **options)\n",
    "#### Parameters:\n",
    "#### test_size — This parameter decides the size of the data that has to be split as the test dataset. This is given as a fraction. For example, if you pass 0.5 as the value, the dataset will be split 50% as the test dataset. If you’re specifying this parameter, you can ignore the next parameter. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n",
    "\n",
    "#### train_size — You have to specify this parameter only if you’re not specifying the test_size. This is the same as test_size, but instead you tell the class what percent of the dataset you want to split as the training set.\n",
    "\n",
    "#### random_state — Here you pass an integer, which will act as the seed for the random number generator during the split. Or, you can also pass an instance of the RandomState class, which will become the number generator. If you don’t pass anything, the RandomState instance used by np.random will be used instead.\n",
    "\n",
    "#### shuffle —  Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Splitting the dataset using cross validation technique\n",
    "\n",
    "#### The validation set approach\n",
    "In this approach, we reserve 50% of the dataset for validation and the remaining 50% for model training. \n",
    "#### Disadvantage\n",
    "since we are training a model on only 50% of the dataset, there is a huge possibility that we might miss out on some interesting information about the data which will lead to a higher bias.\n",
    "\n",
    "\n",
    "#### Leave one out cross validation (LOOCV)\n",
    "In this approach, we reserve only one data point from the available dataset, and train the model on the rest of the data. This process iterates for each data point.\n",
    "#### Disadvantage\n",
    "1) We make use of all data points, hence the bias will be low\n",
    "2) We repeat the cross validation process n times (where n is number of data points) which results in a higher execution time\n",
    "3) This approach leads to higher variation in testing model effectiveness because we test against one data point. So, our estimation gets highly influenced by the data point. If the data point turns out to be an outlier, it can lead to a higher variation.\n",
    "\n",
    "#### k-fold cross validation\n",
    "From the above two validation methods, we’ve learnt:\n",
    "\n",
    "1) We should train the model on a large portion of the dataset. Otherwise we’ll fail to read and recognise the underlying trend in the data. This will eventually result in a higher bias\n",
    "\n",
    "2) We also need a good ratio of testing data points. As we have seen above, less amount of data points can lead to a variance error while testing the effectiveness of the model\n",
    "\n",
    "3) We should iterate on the training and testing process multiple times. We should change the train and test dataset distribution. This helps in validating the model effectiveness properly\n",
    "Do we have a method which takes care of all these 3 requirements?\n",
    "\n",
    "Yes! That method is known as “k-fold cross validation”. It’s easy to follow and implement. Below are the steps for it:\n",
    "\n",
    "1) Randomly split your entire dataset into k”folds”\n",
    "\n",
    "2) For each k-fold in your dataset, build your model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n",
    "\n",
    "3) Record the error you see on each of the predictions\n",
    "\n",
    "4) Repeat this until each of the k-folds has served as the test set\n",
    "\n",
    "5) The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model\n",
    "\n",
    "#### “How to choose the right value of k?”.\n",
    "\n",
    "Always remember, a lower value of k is more biased, and hence undesirable. On the other hand, a higher value of K is less biased, but can suffer from large variability. It is important to know that a smaller value of k always takes us towards validation set approach, whereas a higher value of k leads to LOOCV approach.\n",
    "\n",
    "#### class sklearn.model_selection.KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "#### parameters:\n",
    "#### n_splits: \n",
    "Number of folds. default is 5.\n",
    "\n",
    "#### shuffle:\n",
    "Whether to shuffle the data before splitting into batches.\n",
    "\n",
    "#### random_state:\n",
    "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Only used when shuffle is True. This should be left to None if shuffle is False.\n",
    "\n",
    "To calculate error metrics of KFolds of test sets:\n",
    "model_selection.cross_val_score(model, X, y, cv=kf, scoring=‘neg_mean_absolute_error’)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
    "    y_train_cv, y_test_cv = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Text vectorization\n",
    "The dataset contain numerical value, string value, character value, categorical value,etc. Conversion of these types of features into numerical feature is called featurization or text vectorization. \n",
    "\n",
    "Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. \n",
    "The different types of word embeddings can be broadly classified into two categories-\n",
    "\n",
    "Frequency based Embedding\n",
    "\n",
    "Prediction based Embedding\n",
    "\n",
    "#### Frequency based Embedding:\n",
    "There are generally three types of vectors that we encounter under this category.\n",
    "\n",
    "1) Count Vector\n",
    "2) TF-IDF Vector\n",
    "3) Co-Occurrence Vector\n",
    "\n",
    "#### Prediction based vector:\n",
    "1) Continuous bag of words (CBOW)\n",
    "2) Skip- Gram model\n",
    "\n",
    "#### Using Pre-trained word vectors:\n",
    "1) Word2Vec\n",
    "2) Fasttext\n",
    "3) Glove\n",
    "\n",
    "\n",
    "#### 1) Bag of words:\n",
    "Its called bag of words because any order of the words in the document is discarded it only tells us weather word is present in the document or not. \n",
    "\n",
    "“There used to be Stone Age”\n",
    "\n",
    "“There used to be Bronze Age”\n",
    "\n",
    "“There used to be Iron Age”\n",
    "\n",
    "“There was Age of Revolution”\n",
    "\n",
    "“Now it is Digital Age”\n",
    "\n",
    "Here each sentence is separate document if we make list of the word such that one word should be occur only once than our list looks like as follow:\n",
    "\n",
    "“There”,”was”,”to”,”be”,”used”,”Stone”,”Bronze,”Iron”,”Revolution”,”Digital”,”Age”,”of”,”Now”,”it”,”is”\n",
    "\n",
    "So how a word can be converted to vector can be understood by simple word count example where we count occurrence of word in a document w.r.t list. For example- vector conversion of sentence “There used to be Stone Age” can be represented as :\n",
    "\n",
    "“There” = 1\n",
    "\n",
    "”was”= 0\n",
    "\n",
    "”to”= 1\n",
    "\n",
    "”be” =1\n",
    "\n",
    "”used” = 1\n",
    "\n",
    "”Stone”= 1\n",
    "\n",
    "”Bronze” =0\n",
    "\n",
    "“Iron” =0\n",
    "\n",
    "”Revolution”= 0\n",
    "\n",
    "”Digital”= 0\n",
    "\n",
    "”Age”=1\n",
    "\n",
    "”of”=0\n",
    "\n",
    "”Now”=0\n",
    "\n",
    "”it”=0\n",
    "\n",
    "”is”=0\n",
    "\n",
    "So here we basically convert word into vector . By following same approach other vector value are as follow:\n",
    "\n",
    "“There used to be bronze age” = [1,0,1,1,1,0,1,0,0,0,1,0,0,0,0]\n",
    "\n",
    "“There used to be iron age” = [1,0,1,1,1,0,0,1,0,0,1,0,0,0,0]\n",
    "\n",
    "“There was age of revolution” = [1,1,0,0,0,0,0,0,1,0,1,1,0,0,0]\n",
    "\n",
    "“Now its digital Age” = [0,0,0,0,0,0,0,0,0,1,1,0,1,1,1]\n",
    "\n",
    "The approach which is discussed above is unigram because we are considering only one word at a time . Similarly we have bigram(using two words at a time- for example — There used, Used to, to be, be Stone, Stone age), trigram(using three words at a time- for example- there used to, used to be ,to be Stone,be Stone Age), ngram(using n words at a time)\n",
    "\n",
    "By using CountVectorizer function we can convert text document to matrix of word count. Matrix which is produced here is sparse matrix. By using CountVectorizer on above document we get 5*15 sparse matrix of type numpy.int64.\n",
    "\n",
    "After applying the CountVectorizer we can map each word to feature indices as shown below:\n",
    "<img src=\"images/feature_indices_mapping.png\">\n",
    "\n",
    "This can be transformed into sparse matrix by using as shown below:\n",
    "<img src=\"images/sparse_matrix.png\">\n",
    "\n",
    "Countvectorizer produces sparse matrix which sometime not suited for some machine learning model hence first convert this sparse matrix to dense matrix then apply machine learning model.\n",
    "\n",
    "#### sklearn.feature_extraction.text.CountVectorizer() \n",
    "#### parameters:\n",
    "#### token_pattern: string\n",
    "Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "#### ngram_rangetuple:(min_n, max_n), default=(1, 1)\n",
    "The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable.\n",
    "\n",
    "#### analyzer: string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
    "Whether the feature should be made of word n-gram or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "#### 2) TF-IDF:\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency which basically tells importance of the word in the corpus or dataset. TF-IDF contain two concept Term Frequency(TF) and Inverse Document Frequency(IDF).\n",
    "\n",
    "Term Frequency is defined as how frequently the word appear in the document or corpus. As each sentence is not the same length so it may be possible a word appears in long sentence occur more time as compared to word appear in sorter sentence. Term frequency can be defined as:\n",
    "<img src=\"images/tf.png\">\n",
    "\n",
    "Suppose we have sentence “The TFIDF Vectorization Process is Beautiful Concept” and we have to find the find frequency count of these words in five different documents\n",
    "<img src=\"images/tf2.png\">\n",
    "\n",
    "As shown in Table 1 frequency of ‘The’ is maximum in every Document. Suppose frequency of ‘The’ in Document6 is 2 million while frequency of ‘The’ in Document7 in 3 million. Frequency of ‘The’ is very large in Document6 and Document7 so we can add log term to reduce the value of frequency count (log(2 million) =21). Adding log not only dampen the performance of idf but also reduce the frequency count of TF. Hence formula of TF can be defined as:\n",
    "<img src=\"images/tf3.png\">\n",
    "\n",
    "When tf = 1 log term will become zero and value will become 1 . Adding 1 is just to differentiate between tf=0 and tf =1\n",
    "Hence Table 1 can be modified to :\n",
    "<img src=\"images/tf4.png\">\n",
    "\n",
    "Inverse Document frequency is another concept which is used for finding out importance of the word. It is based on the fact that less frequent words are more informative and important. IDF is represented by formula:\n",
    "<img src=\"images/idf.png\">\n",
    "\n",
    "Let us consider the above example again\n",
    "<img src=\"images/idf2.png\">\n",
    "In Table 3 most frequent word is ‘The’ and ‘is ’ but it is least important according to IDF and the word which appear very less such as ‘TFIDF’, ‘Concept’ are important words. Hence, we can say that IDF of rare term is high and IDF of frequent term is low\n",
    "\n",
    "TF-IDF is basically a multiplication between Table 2 (TF table) and Table 3(IDF table) . It basically reduces values of common word that are used in different document. As we can see that in Table 4 most important word after multiplication of TF and IDF is ‘TFIDF’ while most frequent word such as ‘The’ and ‘is’ are not that important\n",
    "<img src=\"images/tfidf.png\">\n",
    "\n",
    "#### 3) Word2Vec:\n",
    "Word2Vec is Word representations in Vector Space. It is a strategy where words are represented as a bunch of numbers. These numbers(Vectors) are not assigned in random they are assigned in such a way that two similar words are closer together in a vector space.\n",
    "\n",
    "After the words are converted as vectors, we need to use some techniques such as Euclidean distance, Cosine Similarity to identify similar words. word2vec uses cosine similarity for finding out most similar word\n",
    "\n",
    "Why Cosine Similarity?\n",
    "Count the common words or Euclidean distance is the general approach used to match similar documents which are based on counting the number of common words between the documents.\n",
    "This approach will not work even if the number of common words increases but the document talks about different topics. To overcome this flaw, the “Cosine Similarity” approach is used to find the similarity between the documents.\n",
    "<img src=\"images/cosine.png\">\n",
    "Mathematically, it measures the cosine of the angle between two vectors (item1, item2) projected in an N-dimensional vector space. The advantageous of cosine similarity is, it predicts the document similarity even Euclidean is distance.\n",
    "“Smaller the angle, the higher the similarity” — Cosine Similarity.\n",
    "\n",
    "Let’s see an example.\n",
    "\n",
    "Julie loves John more than Linda loves John\n",
    "\n",
    "Jane loves John more than Julie loves John\n",
    "\n",
    "John  2    2\n",
    "\n",
    "Jane  0    1\n",
    "\n",
    "Julie 1    1\n",
    "\n",
    "Linda 1    0\n",
    " \n",
    "likes 0    1\n",
    "\n",
    "loves 2    1\n",
    "\n",
    "more  1    1\n",
    "\n",
    "than  1    1\n",
    "\n",
    "the two vectors are,\n",
    "\n",
    "Item 1: [2, 0, 1, 1, 0, 2, 1, 1]\n",
    "\n",
    "Item 2: [2, 1, 1, 0, 1, 1, 1, 1]\n",
    "\n",
    "The cosine angle (the smaller the angle) between the two vectors' value is 0.822 which is nearest to 1.\n",
    "\n",
    "Now let’s see what are all the ways to convert sentences into vectors.\n",
    "\n",
    "Word embeddings coming from pre-trained methods such as,\n",
    "\n",
    "Word2Vec — From Google\n",
    "\n",
    "Fasttext — From Facebook\n",
    "\n",
    "Glove — From Standford\n",
    "\n",
    "We will use Word2Vec by google. It is deep learning technique with two-layer neural network.Google Word2vec take input from large data (in this scenario we are using google data) and convert into vector space. Google word2vec is basically pretrained on google dataset. Word2vec basically place the word in the feature space is such a way that their location is determined by their meaning i.e. words having similar meaning are clustered together and the distance between two words also have same meaning.  \n",
    "<img src=\"images/w2v.png\">\n",
    "\n",
    "For Google Word2vec we are using google dataset to train the model because not only it cover most of the words. But before using Google word2vec we must install and import genism. Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python.\n",
    "\n",
    "#### Unigram and bigram explained:\n",
    "Example: Consider the sentence \"I ate banana\".\n",
    "\n",
    "In Unigram we assume that the occurrence of each word is independent of its previous word. Hence each word becomes a gram(feature) here.\n",
    "\n",
    "For unigram, we will get 3 features - 'I', 'ate', 'banana' and all 3 are independent of each other. Although this is not the case in real languages.\n",
    "\n",
    "In Bigram we assume that each occurrence of each word depends only on its previous word. Hence two words are counted as one gram(feature) here.\n",
    "\n",
    "For bigram, we will get 2 features - 'I ate' and 'ate banana'. This makes sense since the model will learn that 'banana' comes after 'ate' and not the other way around.\n",
    "\n",
    "Similarly, we can have trigram.......n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words (unigram) \n",
    "# \\w word. matches any word chracter\n",
    "# {1,} quantifier. Match 1 or more of the preceding token\n",
    "count_vectorizer1=CountVectorizer(analyzer='word')\n",
    "count_vectorizer1.fit(X)\n",
    "count_vect1_tr= count_vectorizer1.transform(X_train)\n",
    "count_vect1_te= count_vectorizer1.transform(X_test)\n",
    "\n",
    "count_vect1_tr_dtm= count_vect1_tr.toarray()\n",
    "count_vect1_te_dtm= count_vect1_te.toarray()\n",
    "#print(vectorizer1.get_feature_names())\n",
    "#print(count_vect1.toarray())\n",
    "#print(count_vect1.shape)\n",
    "\n",
    "# Bag of Words (bigram)\n",
    "count_vectorizer2=CountVectorizer(analyzer='word',ngram_range=(2, 2))\n",
    "count_vectorizer2.fit(X)\n",
    "count_vect2_tr= count_vectorizer2.transform(X_train)\n",
    "count_vect2_te= count_vectorizer2.transform(X_test)\n",
    "#print(vectorizer2.get_feature_names())\n",
    "#print(count_vect2.toarray())\n",
    "\n",
    "# Bag of Words (trigram)\n",
    "count_vectorizer3=CountVectorizer(analyzer='word',ngram_range=(3, 3))\n",
    "count_vectorizer3.fit(X)\n",
    "count_vect3_tr= count_vectorizer3.transform(X_train)\n",
    "count_vect3_te= count_vectorizer3.transform(X_test)\n",
    "#print(vectorizer3.get_feature_names())\n",
    "#print(count_vect3.toarray())\n",
    "\n",
    "# Bag of Words (unigram and bigram)\n",
    "count_vectorizer4=CountVectorizer(analyzer='word',ngram_range=(1, 2))\n",
    "count_vectorizer4.fit(X)\n",
    "count_vect4_tr= count_vectorizer4.transform(X_train)\n",
    "count_vect4_te= count_vectorizer4.transform(X_test)\n",
    "#print(vectorizer4.get_feature_names())\n",
    "#print(count_vect4.toarray())\n",
    "\n",
    "# Bag of Words (character level,unigram)\n",
    "count_vectorizer5=CountVectorizer(analyzer='char')\n",
    "count_vectorizer5.fit(X)\n",
    "count_vect5_tr= count_vectorizer5.transform(X_train)\n",
    "count_vect5_te= count_vectorizer5.transform(X_test)\n",
    "#print(vectorizer5.get_feature_names())\n",
    "#print(count_vect5.toarray())\n",
    "\n",
    "# Bag of Words (character level,bigram)\n",
    "count_vectorizer6=CountVectorizer(analyzer='char',ngram_range=(2, 2))\n",
    "count_vectorizer6.fit(X)\n",
    "count_vect6_tr= count_vectorizer6.transform(X_train)\n",
    "count_vect6_te= count_vectorizer6.transform(X_test)\n",
    "#print(vectorizer6.get_feature_names())\n",
    "#print(count_vect6.toarray())\n",
    "\n",
    "# Bag of Words (character level,trigram)\n",
    "count_vectorizer7=CountVectorizer(analyzer='char',ngram_range=(3, 3))\n",
    "count_vectorizer7.fit(X)\n",
    "count_vect7_tr= count_vectorizer7.transform(X_train)\n",
    "count_vect7_te= count_vectorizer7.transform(X_test)\n",
    "#print(vectorizer7.get_feature_names())\n",
    "#print(count_vect7.toarray())\n",
    "\n",
    "# Bag of Words (character level,unigram and bigram)\n",
    "count_vectorizer8=CountVectorizer(analyzer='char',ngram_range=(2, 3))\n",
    "count_vectorizer8.fit(X)\n",
    "count_vect8_tr= count_vectorizer8.transform(X_train)\n",
    "count_vect8_te= count_vectorizer8.transform(X_test)\n",
    "#print(vectorizer8.get_feature_names())\n",
    "#print(count_vect8.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (unigram)\n",
    "tfidf_vectorizer1=TfidfVectorizer(analyzer='word')\n",
    "tfidf_vectorizer1.fit(X)\n",
    "tfidf_vect1_tr= tfidf_vectorizer1.transform(X_train)\n",
    "tfidf_vect1_te= tfidf_vectorizer1.transform(X_test)\n",
    "#print(tfidf_vectorizer1.get_feature_names())\n",
    "#print(count_vect5.toarray())\n",
    "#print(tfidf_vect1.shape)\n",
    "\n",
    "# TF-IDF (bigram)\n",
    "tfidf_vectorizer2=TfidfVectorizer(analyzer='word',ngram_range=(2, 2))\n",
    "tfidf_vectorizer2.fit(X)\n",
    "tfidf_vect2_tr= tfidf_vectorizer2.transform(X_train)\n",
    "tfidf_vect2_te= tfidf_vectorizer2.transform(X_test)\n",
    "#print(tfidf_vectorizer2.get_feature_names())\n",
    "#print(tfidf_vect2.toarray())\n",
    "\n",
    "# TF-IDF (trigram)\n",
    "tfidf_vectorizer3=TfidfVectorizer(analyzer='word',ngram_range=(3, 3))\n",
    "tfidf_vectorizer3.fit(X)\n",
    "tfidf_vect3_tr= tfidf_vectorizer3.transform(X_train)\n",
    "tfidf_vect3_te= tfidf_vectorizer3.transform(X_test)\n",
    "#print(tfidf_vectorizer3.get_feature_names())\n",
    "#print(tfidf_vect3.toarray())\n",
    "\n",
    "# TF-IDF (unigram and bigram)\n",
    "tfidf_vectorizer4=TfidfVectorizer(analyzer='word',ngram_range=(2, 3))\n",
    "tfidf_vectorizer4.fit(X)\n",
    "tfidf_vect4_tr= tfidf_vectorizer4.transform(X_train)\n",
    "tfidf_vect4_te= tfidf_vectorizer4.transform(X_test)\n",
    "#print(tfidf_vectorizer4.get_feature_names())\n",
    "#print(tfidf_vect4.toarray())\n",
    "\n",
    "# TF-IDF (character level,unigram)\n",
    "tfidf_vectorizer5=TfidfVectorizer(analyzer='char')\n",
    "tfidf_vectorizer5.fit(X)\n",
    "tfidf_vect5_tr= tfidf_vectorizer5.transform(X_train)\n",
    "tfidf_vect5_te= tfidf_vectorizer5.transform(X_test)\n",
    "#print(tfidf_vectorizer5.get_feature_names())\n",
    "#print(tfidf_vect5.toarray())\n",
    "\n",
    "# TF-IDF (character level,bigram)\n",
    "tfidf_vectorizer6=TfidfVectorizer(analyzer='char',ngram_range=(2, 2))\n",
    "tfidf_vectorizer6.fit(X)\n",
    "tfidf_vect6_tr= tfidf_vectorizer6.transform(X_train)\n",
    "tfidf_vect6_te= tfidf_vectorizer6.transform(X_test)\n",
    "#print(tfidf_vectorizer6.get_feature_names())\n",
    "#print(tfidf_vect6.toarray())\n",
    "\n",
    "# TF-IDF (character level,trigram)\n",
    "tfidf_vectorizer7=TfidfVectorizer(analyzer='char',ngram_range=(3, 3))\n",
    "tfidf_vectorizer7.fit(X)\n",
    "tfidf_vect7_tr= tfidf_vectorizer7.transform(X_train)\n",
    "tfidf_vect7_te= tfidf_vectorizer7.transform(X_test)\n",
    "#print(tfidf_vectorizer7.get_feature_names())\n",
    "#print(tfidf_vect7.toarray())\n",
    "\n",
    "# TF-IDF (character level,unigram and bigram)\n",
    "tfidf_vectorizer8=TfidfVectorizer(analyzer='char',ngram_range=(2, 3))\n",
    "tfidf_vectorizer8.fit(X)\n",
    "tfidf_vect8_tr= tfidf_vectorizer8.transform(X_train)\n",
    "tfidf_vect8_te= tfidf_vectorizer8.transform(X_test)\n",
    "#print(tfidf_vectorizer8.get_feature_names())\n",
    "#print(tfidf_vect8.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Using fasttext\n",
    "# load the pre-trained word-embedding vectors \n",
    "\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec',encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(X)\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "fstxt_tr = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=70)\n",
    "fstxt_te = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Using google\n",
    "w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(w2v_model.wv.most_similar('flood'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Metrics to Evaluate Machine Learning Algorithm\n",
    "\n",
    "## Refer to this also: https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "\n",
    "#### 1) Classification Accuracy:\n",
    "Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.\n",
    "<img src=\"images/accu.gif\">\n",
    "It works well only if there are equal number of samples belonging to each class.\n",
    "For example, consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A.\n",
    "When the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the test accuracy would drop down to 60%. Classification Accuracy is great, but gives us the false sense of achieving high accuracy.\n",
    "<img src=\"images/accu1.gif\">\n",
    "Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall.\n",
    "\n",
    "#### sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
    "\n",
    "parameters:\n",
    "\n",
    "y_true: 1d array-like, or label indicator array / sparse matrix\n",
    "Ground truth (correct) labels.\n",
    "\n",
    "y_pred: 1d array-like, or label indicator array / sparse matrix\n",
    "Predicted labels, as returned by a classifier.\n",
    "\n",
    "normalize: bool, optional (default=True)\n",
    "If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.\n",
    "\n",
    "sample_weight: array-like of shape (n_samples,), default=None\n",
    "Sample weights.\n",
    "\n",
    "#### 2) Logarithmic Loss:\n",
    "Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. \n",
    "\n",
    "#### sklearn.metrics.log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "\n",
    "parameters:\n",
    "\n",
    "y_true: array-like or label indicator matrix\n",
    "Ground truth (correct) labels for n_samples samples.\n",
    "\n",
    "y_pred: array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
    "Predicted probabilities, as returned by a classifier’s predict_proba method. \n",
    "\n",
    "eps: float\n",
    "Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).\n",
    "\n",
    "normalize: bool, optional (default=True)\n",
    "If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.\n",
    "\n",
    "sample_weight: array-like of shape (n_samples,), default=None\n",
    "Sample weights.\n",
    "\n",
    "labels: array-like, optional (default=None)\n",
    "If not provided, labels will be inferred from y_true. If labels is None and y_pred has shape (n_samples,) the labels are assumed to be binary and are inferred from y_true. .. versionadded:: 0.18\n",
    "\n",
    "\n",
    "#### 3) Confusion Matrix:\n",
    "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.\n",
    "<img src=\"images/confu.png\">\n",
    "\n",
    "#### sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\n",
    "parameter:\n",
    "y_true: array-like of shape (n_samples,)\n",
    "Ground truth (correct) target values.\n",
    "\n",
    "y_pred: array-like of shape (n_samples,)\n",
    "Estimated targets as returned by a classifier.\n",
    "\n",
    "labels: array-like of shape (n_classes), default=None\n",
    "List of labels to index the matrix. This may be used to reorder or select a subset of labels. If None is given, those that appear at least once in y_true or y_pred are used in sorted order.\n",
    "\n",
    "sample_weight: array-like of shape (n_samples,), default=None\n",
    "Sample weights.\n",
    "\n",
    "normalize: {‘true’, ‘pred’, ‘all’}, default=None\n",
    "Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized.\n",
    "\n",
    "#### 4) Area under Curve:\n",
    "Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :\n",
    "True Positive Rate (Sensitivity) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n",
    "<img src=\"images/auc.gif\">\n",
    "False Positive Rate (Specificity) : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n",
    "<img src=\"images/auc1.gif\">\n",
    "False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR bot hare computed at threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1].\n",
    "AUC has a range of [0, 1]. The greater the value, the better is the performance of our model.\n",
    "\n",
    "#### sklearn.metrics.auc(x, y)\n",
    "\n",
    "parameters:\n",
    "xarray, shape = [n]\n",
    "x coordinates. These must be either monotonic increasing or monotonic decreasing.\n",
    "\n",
    "yarray, shape = [n]\n",
    "y coordinates.\n",
    "\n",
    "But for binary classfication we will use ROC AUC (Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores) as in auc the parameters are x and y which should be inputed.\n",
    "\n",
    "#### sklearn.metrics.roc_auc_score(y_true, y_score, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
    "\n",
    "parameters:\n",
    "\n",
    "max_fpr: float > 0 and <= 1, default=None\n",
    "If not None, the standardized partial AUC [2] over the range [0, max_fpr] is returned. For the multiclass case, max_fpr, should be either equal to None or 1.0 as AUC ROC partial computation currently is not supported for multiclass.\n",
    "\n",
    "multi_class: {‘raise’, ‘ovr’, ‘ovo’}, default=’raise’\n",
    "Multiclass only. Determines the type of configuration to use. The default value raises an error, so either 'ovr' or 'ovo' must be passed explicitly.\n",
    "\n",
    "#### 5) F1 Score:\n",
    "What percent of positive predictions were correct? \n",
    "F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
    "The greater the F1 Score, the better is the performance of our model.\n",
    "<img src=\"images/f1.gif\">\n",
    "\n",
    "#### sklearn.metrics.f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "\n",
    "parameters:\n",
    "y_true: 1d array-like, or label indicator array / sparse matrix\n",
    "Ground truth (correct) target values.\n",
    "\n",
    "y_pred: 1d array-like, or label indicator array / sparse matrix\n",
    "Estimated targets as returned by a classifier.\n",
    "\n",
    "labels: list, optional\n",
    "The set of labels to include when average != 'binary', and their order if average is None.\n",
    "\n",
    "pos_label: str or int, 1 by default\n",
    "The class to report if average='binary' and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting labels=[pos_label] and average != 'binary' will report scores for that label only.\n",
    "\n",
    "average: string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]\n",
    "This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. \n",
    "\n",
    "sample_weight: array-like of shape (n_samples,), default=None\n",
    "Sample weights.\n",
    "\n",
    "zero_division: “warn”, 0 or 1, default=”warn”\n",
    "Sets the value to return when there is a zero division. If set to “warn”, this acts as 0, but warnings are also raised.\n",
    "\n",
    "#### 6) Precision:\n",
    "What percent of your predictions were correct? \n",
    "Precision is the ability of a classifier not to label an instance positive that is actually negative. High precision relates to the low false positive rate\n",
    "<img src=\"images/prec.gif\">\n",
    "#### sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn') \n",
    "\n",
    "\n",
    "#### 7) Recall:\n",
    "What percent of the positive cases did you catch? \n",
    "Recall is the ability of a classifier to find all positive instances.\n",
    "<img src=\"images/reca.gif\">\n",
    "\n",
    "#### sklearn.metrics.recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "\n",
    "#### 8) Mean Absolute Error:\n",
    "Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as :\n",
    "<img src=\"images/mae.gif\">\n",
    "\n",
    "#### sklearn.metrics.mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "\n",
    "parameters:\n",
    "\n",
    "y_true: array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "Ground truth (correct) target values.\n",
    "\n",
    "y_pred: array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "Estimated target values.\n",
    "\n",
    "sample_weight: array-like of shape (n_samples,), optional\n",
    "Sample weights.\n",
    "\n",
    "multioutput: string in [‘raw_values’, ‘uniform_average’]\n",
    "or array-like of shape (n_outputs) Defines aggregating of multiple output values. Array-like value defines weights used to average errors.\n",
    "\n",
    "‘raw_values’ :\n",
    "Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "‘uniform_average’ :\n",
    "Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "\n",
    "#### 9) Mean Squared Error:\n",
    "Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.\n",
    "<img src=\"images/mse.gif\">\n",
    "\n",
    "#### sklearn.metrics.mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average', squared=True)\n",
    "\n",
    "parameters:\n",
    "squared: boolean value, optional (default = True)\n",
    "If True returns MSE value, if False returns RMSE value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Model Building\n",
    "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n",
    "\n",
    "Naive Bayes Classifier: \n",
    "\n",
    "Linear Classifier\n",
    "\n",
    "Support Vector Machine\n",
    "\n",
    "Bagging Models\n",
    "\n",
    "Boosting Models\n",
    "\n",
    "Shallow Neural Networks\n",
    "\n",
    "Deep Neural Networks\n",
    "\n",
    "Convolutional Neural Network (CNN)\n",
    "\n",
    "Long Short Term Modelr (LSTM)\n",
    "\n",
    "Gated Recurrent Unit (GRU)\n",
    "\n",
    "Bidirectional RNN\n",
    "\n",
    "Recurrent Convolutional Neural Network (RCNN)\n",
    "\n",
    "Other Variants of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return (metrics.accuracy_score(y_test, predictions), metrics.confusion_matrix(y_test, predictions),metrics.precision_score(y_test, predictions,average='weighted'),metrics.f1_score(y_test, predictions,average='weighted'),metrics.recall_score(y_test, predictions,average='weighted'),metrics.roc_auc_score(y_test, predictions), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1) Naive Bayes Classifier: \n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
    "<img src=\"images/baye.png\">\n",
    "The different types are:\n",
    "1. Gaussian Naive Bayes:\n",
    "Implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "Implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). Therfore we will use this.\n",
    "\n",
    "3. Complement Naive Bayes:\n",
    "Implements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets.\n",
    "\n",
    "4. Bernoulli Naive Bayes:\n",
    "Implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).\n",
    "\n",
    "5. Categorical Naive Bayes:\n",
    "Implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index , has its own categorical distribution.\n",
    "\n",
    "6. Out-of-core naive Bayes model fitting:\n",
    "Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit method that can be used incrementally as done with other classifiers.\n",
    "\n",
    "#### sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None) \n",
    "\n",
    "#### Parameters:\n",
    "alpha: float, optional (default=1.0)\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "fit_prior: boolean, optional (default=True)\n",
    "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "class_prior: array-like, size (n_classes,), optional (default=None)\n",
    "Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "from sklearn import naive_bayes, metrics\n",
    "\n",
    "# Bag of Words (unigram) \n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect1_tr, y_train, count_vect1_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect2_tr, y_train, count_vect2_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect3_tr, y_train, count_vect3_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect4_tr, y_train, count_vect4_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect5_tr, y_train, count_vect5_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect6_tr, y_train, count_vect6_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect7_tr, y_train, count_vect7_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), count_vect8_tr, y_train, count_vect8_te)\n",
    "print(\"Multinomial Naive Bayes,Bag of Words (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect1_tr, y_train, tfidf_vect1_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect2_tr, y_train, tfidf_vect2_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect3_tr, y_train, tfidf_vect3_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect4_tr, y_train, tfidf_vect4_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect5_tr, y_train, tfidf_vect5_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect6_tr, y_train, tfidf_vect6_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect7_tr, y_train, tfidf_vect7_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(naive_bayes.MultinomialNB(), tfidf_vect8_tr, y_train, tfidf_vect8_te)\n",
    "print(\"Multinomial Naive Bayes,TF-IDF (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2) Linear Classifier\n",
    "\n",
    "Implementing a Linear Classifier (Logistic Regression)\n",
    "\n",
    "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. \n",
    "\n",
    "#### sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None) \n",
    "\n",
    "parameters:\n",
    "\n",
    "This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers.\n",
    "\n",
    "penalty: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’\n",
    "Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver. If ‘none’ (not supported by the liblinear solver), no regularization is applied.\n",
    "\n",
    "\n",
    "dual: bool, default=False\n",
    "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n",
    "\n",
    "tol: float, default=1e-4\n",
    "Tolerance for stopping criteria.\n",
    "\n",
    "C: float, default=1.0\n",
    "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "fit_intercept: bool, default=True\n",
    "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.\n",
    "\n",
    "intercept_scaling: float, default=1\n",
    "Useful only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic_feature_weight.\n",
    "\n",
    "class_weight: dict or ‘balanced’, default=None\n",
    "Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "\n",
    "random_state: int, RandomState instance, default=None\n",
    "The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when solver == ‘sag’ or ‘liblinear’.\n",
    "\n",
    "solver: {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "max_iter: int, default=100\n",
    "Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class: {‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’\n",
    "If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.\n",
    "\n",
    "verbose: int, default=0\n",
    "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.\n",
    "\n",
    "warm_start: bool, default=False\n",
    "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. \n",
    "\n",
    "n_jobs: int, default=None\n",
    "Number of CPU cores used when parallelizing over classes if multi_class=’ovr’”. This parameter is ignored when the solver is set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or not. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
    "\n",
    "l1_ratio: float, default=None\n",
    "The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'`. Setting ``l1_ratio=0 is equivalent to using penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'. For 0 < l1_ratio <1, the penalty is a combination of L1 and L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Bag of Words (unigram) \n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect1_tr, y_train, count_vect1_te)\n",
    "print(\"Logistic regression,Bag of Words (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect2_tr, y_train, count_vect2_te)\n",
    "print(\"Logistic regression,Bag of Words (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect3_tr, y_train, count_vect3_te)\n",
    "print(\"Logistic regression,Bag of Words (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect4_tr, y_train, count_vect4_te)\n",
    "print(\"Logistic regression,Bag of Words (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect5_tr, y_train, count_vect5_te)\n",
    "print(\"Logistic regression,Bag of Words (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect6_tr, y_train, count_vect6_te)\n",
    "print(\"Logistic regression,Bag of Words (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect7_tr, y_train, count_vect7_te)\n",
    "print(\"Logistic regression,Bag of Words (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs',max_iter=500), count_vect8_tr, y_train, count_vect8_te)\n",
    "print(\"Logistic regression,Bag of Words (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect1_tr, y_train, tfidf_vect1_te)\n",
    "print(\"Logistic regression,TF-IDF (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect2_tr, y_train, tfidf_vect2_te)\n",
    "print(\"Logistic regression,TF-IDF (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect3_tr, y_train, tfidf_vect3_te)\n",
    "print(\"Logistic regression,TF-IDF (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect4_tr, y_train, tfidf_vect4_te)\n",
    "print(\"Logistic regression,TF-IDF (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect5_tr, y_train, tfidf_vect5_te)\n",
    "print(\"Logistic regression,TF-IDF (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect6_tr, y_train, tfidf_vect6_te)\n",
    "print(\"Logistic regression,TF-IDF (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect7_tr, y_train, tfidf_vect7_te)\n",
    "print(\"Logistic regression,TF-IDF (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(linear_model.LogisticRegression(solver='lbfgs'), tfidf_vect8_tr, y_train, tfidf_vect8_te)\n",
    "print(\"Logistic regression,TF-IDF (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3) Support Vector Machine (SVM)\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "<img src=\"images\\svm.jpg\">\n",
    "\n",
    "#### sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "\n",
    "parameters:\n",
    "\n",
    "C: float, optional (default=1.0)\n",
    "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "kernel: string, optional (default=’rbf’)\n",
    "Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’ (text classification) , ‘poly’(image processing), ‘rbf’(radial basis function) , ‘sigmoid’(proxy for neural netwokr), ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n",
    "\n",
    "degree: int, optional (default=3)\n",
    "Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "\n",
    "gamma: {‘scale’, ‘auto’} or float, optional (default=’scale’)\n",
    "Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "\n",
    "if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n",
    "\n",
    "if ‘auto’, uses 1 / n_features.\n",
    "\n",
    "coef0: float, optional (default=0.0)\n",
    "Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "\n",
    "shrinking: boolean, optional (default=True)\n",
    "Whether to use the shrinking heuristic.\n",
    "\n",
    "probability: boolean, optional (default=False)\n",
    "Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation, and predict_proba may be inconsistent with predict.\n",
    "\n",
    "tol: float, optional (default=1e-3)\n",
    "Tolerance for stopping criterion.\n",
    "\n",
    "cache_size: float, optional\n",
    "Specify the size of the kernel cache (in MB).\n",
    "\n",
    "class_weight: {dict, ‘balanced’}, optional\n",
    "Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "verbose: bool, default: False\n",
    "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.\n",
    "\n",
    "max_iter: int, optional (default=-1)\n",
    "Hard limit on iterations within solver, or -1 for no limit.\n",
    "\n",
    "decision_function_shape: ‘ovo’, ‘ovr’, default=’ovr’\n",
    "Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (‘ovo’) is always used as multi-class strategy.\n",
    "\n",
    "break_ties: bool, optional (default=False)\n",
    "If true, decision_function_shape='ovr', and number of classes > 2, predict will break ties according to the confidence values of decision_function; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict.\n",
    "\n",
    "\n",
    "random_state: int, RandomState instance or None, optional (default=None)\n",
    "The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "\n",
    "# Bag of Words (unigram) \n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect1_tr, y_train, count_vect1_te)\n",
    "print(\"SVM,Bag of Words (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect2_tr, y_train, count_vect2_te)\n",
    "print(\"SVM,Bag of Words (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect3_tr, y_train, count_vect3_te)\n",
    "print(\"SVM,Bag of Words (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect4_tr, y_train, count_vect4_te)\n",
    "print(\"SVM,Bag of Words (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect5_tr, y_train, count_vect5_te)\n",
    "print(\"SVM,Bag of Words (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect6_tr, y_train, count_vect6_te)\n",
    "print(\"SVM,Bag of Words (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect7_tr, y_train, count_vect7_te)\n",
    "print(\"SVM,Bag of Words (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), count_vect8_tr, y_train, count_vect8_te)\n",
    "print(\"SVM,Bag of Words (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect1_tr, y_train, tfidf_vect1_te)\n",
    "print(\"SVM,TF-IDF (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect2_tr, y_train, tfidf_vect2_te)\n",
    "print(\"SVM,TF-IDF (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect3_tr, y_train, tfidf_vect3_te)\n",
    "print(\"SVM,TF-IDF (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect4_tr, y_train, tfidf_vect4_te)\n",
    "print(\"SVM,TF-IDF (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect5_tr, y_train, tfidf_vect5_te)\n",
    "print(\"SVM,TF-IDF (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect6_tr, y_train, tfidf_vect6_te)\n",
    "print(\"SVM,TF-IDF (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect7_tr, y_train, tfidf_vect7_te)\n",
    "print(\"SVM,TF-IDF (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(svm.SVC(kernel='linear'), tfidf_vect8_tr, y_train, tfidf_vect8_te)\n",
    "print(\"SVM,TF-IDF (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4) Ensemble methods\n",
    "\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "Examples: Bagging methods, Forests of randomized trees\n",
    "\n",
    "By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "Examples: AdaBoost, Gradient Tree Boosting, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.1) Bagging Models\n",
    "\n",
    "We will be implementing a Random Forest Model. Random Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family.  A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n",
    "\n",
    "#### sklearn.ensemble.RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "parameters:\n",
    "\n",
    "n_estimators: integer, optional (default=100)\n",
    "The number of trees in the forest.\n",
    "\n",
    "criterion: string, optional (default=”gini”)\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "\n",
    "max_depth: integer or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "min_samples_split: int, float, optional (default=2)\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "If int, then consider min_samples_split as the minimum number.\n",
    "\n",
    "If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "min_samples_leaf: int, float, optional (default=1)\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "min_weight_fraction_leaf: float, optional (default=0.)\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "max_features: int, float, string or None, optional (default=”auto”)\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "max_leaf_nodes: int or None, optional (default=None)\n",
    "Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "min_impurity_decrease: float, optional (default=0.)\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "min_impurity_split: float, (default=1e-7)\n",
    "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "bootstrap: boolean, optional (default=True)\n",
    "Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n",
    "\n",
    "oob_score: bool (default=False)\n",
    "Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "n_jobs: int or None, optional (default=None)\n",
    "The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. \n",
    "\n",
    "random_state: int, RandomState instance or None, optional (default=None)\n",
    "Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True) and the sampling of the features to consider when looking for the best split at each node (if max_features < n_features). \n",
    "\n",
    "verbose: int, optional (default=0)\n",
    "Controls the verbosity when fitting and predicting.\n",
    "\n",
    "warm_start: bool, optional (default=False)\n",
    "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See the Glossary.\n",
    "\n",
    "class_weight: dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None)\n",
    "Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "ccp_alpha: non-negative float, optional (default=0.0)\n",
    "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. \n",
    "\n",
    "max_samples: int or float, default=None\n",
    "If bootstrap is True, the number of samples to draw from X to train each base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn import ensemble\n",
    "\n",
    "# Bag of Words (unigram) \n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect1_tr, y_train, count_vect1_te)\n",
    "print(\"Random Forest,Bag of Words (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect2_tr, y_train, count_vect2_te)\n",
    "print(\"Random Forest,Bag of Words (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect3_tr, y_train, count_vect3_te)\n",
    "print(\"Random Forest,Bag of Words (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect4_tr, y_train, count_vect4_te)\n",
    "print(\"Random Forest,Bag of Words (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect5_tr, y_train, count_vect5_te)\n",
    "print(\"Random Forest,Bag of Words (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect6_tr, y_train, count_vect6_te)\n",
    "print(\"Random Forest,Bag of Words (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect7_tr, y_train, count_vect7_te)\n",
    "print(\"Random Forest,Bag of Words (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), count_vect8_tr, y_train, count_vect8_te)\n",
    "print(\"Random Forest,Bag of Words (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect1_tr, y_train, tfidf_vect1_te)\n",
    "print(\"Random Forest,TF-IDF (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect2_tr, y_train, tfidf_vect2_te)\n",
    "print(\"Random Forest,TF-IDF (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect3_tr, y_train, tfidf_vect3_te)\n",
    "print(\"Random Forest,TF-IDF (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect4_tr, y_train, tfidf_vect4_te)\n",
    "print(\"Random Forest,TF-IDF (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect5_tr, y_train, tfidf_vect5_te)\n",
    "print(\"Random Forest,TF-IDF (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect6_tr, y_train, tfidf_vect6_te)\n",
    "print(\"Random Forest,TF-IDF (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect7_tr, y_train, tfidf_vect7_te)\n",
    "print(\"Random Forest,TF-IDF (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(ensemble.RandomForestClassifier(), tfidf_vect8_tr, y_train, tfidf_vect8_te)\n",
    "print(\"Random Forest,TF-IDF (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.2) Boosting Models\n",
    "\n",
    "Boosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing).\n",
    "\n",
    "We will be implementing Xtereme Gradient Boosting Model.\n",
    "\n",
    "General Parameters: \n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "booster: [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "gbtree: tree-based models\n",
    "gblinear: linear models\n",
    "\n",
    "silent: [default=0]\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed.\n",
    "It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "nthread: [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered\n",
    "If you wish to run on all cores, value should not be entered and algorithm will detect automatically\n",
    "\n",
    "Booster Parameters:\n",
    "Though there are 2 types of boosters, I’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "eta: [default=0.3]\n",
    "Analogous to learning rate in GBM\n",
    "Makes the model more robust by shrinking the weights on each step\n",
    "Typical final values to be used: 0.01-0.2\n",
    "\n",
    "min_child_weight: [default=1]\n",
    "Defines the minimum sum of weights of all observations required in a child.\n",
    "Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "max_depth: [default=6]\n",
    "The maximum depth of a tree\n",
    "Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "Should be tuned using CV.\n",
    "Typical values: 3-10\n",
    "\n",
    "max_leaf_nodes:\n",
    "The maximum number of terminal nodes or leaves in a tree.\n",
    "Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "gamma: [default=0]\n",
    "A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "max_delta_step: [default=0]\n",
    "In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "\n",
    "subsample: [default=1]\n",
    "Denotes the fraction of observations to be randomly samples for each tree.\n",
    "Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "Typical values: 0.5-1\n",
    "\n",
    "colsample_bytree: [default=1]\n",
    "Denotes the fraction of columns to be randomly samples for each tree.\n",
    "Typical values: 0.5-1\n",
    "colsample_bylevel [default=1]\n",
    "Denotes the subsample ratio of columns for each split, in each level.\n",
    "\n",
    "lambda: [default=1]\n",
    "L2 regularization term on weights (analogous to Ridge regression)\n",
    "This used to handle the regularization part of XGBoost. \n",
    "\n",
    "alpha: [default=0]\n",
    "L1 regularization term on weight (analogous to Lasso regression)\n",
    "Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "\n",
    "scale_pos_weight: [default=1]\n",
    "A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtereme Gradient Boosting Model\n",
    "import xgboost\n",
    "\n",
    "# Bag of Words (unigram) \n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect1_tr, y_train, count_vect1_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect2_tr, y_train, count_vect2_te)\n",
    "print(\"Xtereme Gradient Boosting Modelt,Bag of Words (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect3_tr, y_train, count_vect3_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect4_tr, y_train, count_vect4_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect5_tr, y_train, count_vect5_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect6_tr, y_train, count_vect6_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect7_tr, y_train, count_vect7_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# Bag of Words (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), count_vect8_tr, y_train, count_vect8_te)\n",
    "print(\"Xtereme Gradient Boosting Model,Bag of Words (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect1_tr, y_train, tfidf_vect1_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect2_tr, y_train, tfidf_vect2_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect3_tr, y_train, tfidf_vect3_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect4_tr, y_train, tfidf_vect4_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect5_tr, y_train, tfidf_vect5_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (character level,unigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect6_tr, y_train, tfidf_vect6_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (character level,bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,trigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect7_tr, y_train, tfidf_vect7_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (character level,trigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)\n",
    "\n",
    "# TF-IDF (character level,unigram and bigram)\n",
    "(accuracy,cm, precision,f1, recall, auc, model) = train_model(xgboost.XGBClassifier(), tfidf_vect8_tr, y_train, tfidf_vect8_te)\n",
    "print(\"Xtereme Gradient Boosting Model,TF-IDF (character level,unigram and bigram): \")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"Confusion matrix:\\n \", cm)\n",
    "print (\"Precision: \", precision)\n",
    "print (\"F1 score: \", f1)\n",
    "print (\"Recall: \", recall)\n",
    "print (\"Area under curve: \", auc)\n",
    "print (\"--\" *50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model1=linear_model.LogisticRegression(solver='lbfgs',max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(count_vect4_tr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model1, 'model1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"Some Flee, Others Restock Before Australian Wildfires Worsen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = count_vectorizer4.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=count_vectorizer4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model1.predict(simple_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
